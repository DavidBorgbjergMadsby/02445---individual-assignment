import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import ttest_rel

# Load the dataset
file_path = "C:\\Users\\david\\OneDrive\\Skrivebord\\DTU- 2. Semestre\\02445 Projekt i statistisk evaluering for Kunstig intelligens og Data\\HR_data.csv"
df = pd.read_csv(file_path)

# Show basic statistics and first rows of the dataset
print(df.describe())
print(df.head())

# Define features and target variable
features = ['HR_Mean', 'HR_Median', 'HR_std', 'HR_Min', 'HR_Max', 'HR_AUC']
target = 'Frustrated'

# Standardize the features
X = df[features]
y = df[target]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a new dataframe with scaled features and original metadata
df_scaled = pd.DataFrame(X_scaled, columns=features)
df_scaled['Individual'] = df['Individual']
df_scaled['Cohort'] = df['Cohort']
df_scaled['Frustrated'] = y

# Initialize lists to store evaluation metrics and predictions
individuals = df_scaled['Individual'].unique()
rf_mae_list, dt_mae_list = [], []
rf_rmse_list, dt_rmse_list = [], []
rf_r2_list, dt_r2_list = [], []
rf_preds, dt_preds, y_true = [], [], []

# Leave-One-Subject-Out Cross-Validation (LOSOCV)
for ind in individuals:
    # Split training and test data by individual
    train = df_scaled[df_scaled['Individual'] != ind]
    test = df_scaled[df_scaled['Individual'] == ind]

    X_train, y_train = train[features], train['Frustrated']
    X_test, y_test = test[features], test['Frustrated']

    # Initialize models
    rf = RandomForestRegressor(n_estimators=100, min_samples_split=5, random_state=42)
    dt = DecisionTreeRegressor(max_depth=5, min_samples_leaf=3, random_state=42)

    # Train models
    rf.fit(X_train, y_train)
    dt.fit(X_train, y_train)

    # Make predictions on test data
    rf_pred = rf.predict(X_test)
    dt_pred = dt.predict(X_test)

    # Store predictions and true values
    rf_preds.extend(rf_pred)
    dt_preds.extend(dt_pred)
    y_true.extend(y_test)

    # Calculate and store evaluation metrics for this fold
    rf_mae_list.append(mean_absolute_error(y_test, rf_pred))
    dt_mae_list.append(mean_absolute_error(y_test, dt_pred))
    rf_rmse_list.append(np.sqrt(mean_squared_error(y_test, rf_pred)))
    dt_rmse_list.append(np.sqrt(mean_squared_error(y_test, dt_pred)))
    rf_r2_list.append(r2_score(y_test, rf_pred))
    dt_r2_list.append(r2_score(y_test, dt_pred))

# Calculate average MAE per cohort
cohorts = df_scaled['Cohort'].unique()
cohort_mae = {c: {'rf': [], 'dt': []} for c in cohorts}

for c in cohorts:
    inds_in_cohort = df_scaled[df_scaled['Cohort'] == c]['Individual'].unique()
    for ind in inds_in_cohort:
        idx = np.where(individuals == ind)[0][0]  # Find index of individual in list
        cohort_mae[c]['rf'].append(rf_mae_list[idx])
        cohort_mae[c]['dt'].append(dt_mae_list[idx])

rf_mae_cohort = [np.mean(cohort_mae[c]['rf']) for c in cohorts]
dt_mae_cohort = [np.mean(cohort_mae[c]['dt']) for c in cohorts]

# Plot bar chart of MAE per cohort for both models
x = np.arange(len(cohorts))
width = 0.35

plt.figure(figsize=(7, 5))
plt.bar(x - width/2, rf_mae_cohort, width, label='Random Forest')
plt.bar(x + width/2, dt_mae_cohort, width, label='Decision Tree')
plt.ylabel('Mean Absolute Error')
plt.title('MAE per Cohort')
plt.xticks(x, cohorts)
plt.legend()
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Summarize results: mean ± std for all metrics and p-values for paired t-tests
results = {
    'Metric': ['MAE', 'RMSE', 'R²'],
    'Random Forest': [
        f"{np.mean(rf_mae_list):.2f} ± {np.std(rf_mae_list):.2f}",
        f"{np.mean(rf_rmse_list):.2f} ± {np.std(rf_rmse_list):.2f}",
        f"{np.mean(rf_r2_list):.2f} ± {np.std(rf_r2_list):.2f}"
    ],
    'Decision Tree': [
        f"{np.mean(dt_mae_list):.2f} ± {np.std(dt_mae_list):.2f}",
        f"{np.mean(dt_rmse_list):.2f} ± {np.std(dt_rmse_list):.2f}",
        f"{np.mean(dt_r2_list):.2f} ± {np.std(dt_r2_list):.2f}"
    ],
    'p-value': [
        f"{ttest_rel(rf_mae_list, dt_mae_list).pvalue:.4f}",
        f"{ttest_rel(rf_rmse_list, dt_rmse_list).pvalue:.4f}",
        f"{ttest_rel(rf_r2_list, dt_r2_list).pvalue:.4f}"
    ]
}

metrics_df = pd.DataFrame(results)
print(metrics_df)

# Boxplot comparing MAE distributions of the two models
plt.figure(figsize=(6, 5))
plt.boxplot([rf_mae_list, dt_mae_list], labels=['Random Forest', 'Decision Tree'])
plt.title("MAE per Model (LOSOCV)")
plt.ylabel("Mean Absolute Error")
plt.grid(True)
plt.tight_layout()
plt.show()
